Influence Qs

- What is the min. no. train examples to remove to flip this prediction?
	Basically like a counterfactual explanation for instance-influence methods;
	perhaps something like a "counterfactual influence set" of train examples?

- Can we identify the most important group/cluster of train examples
	for a given test examples?

- What if you have a grad(0) for a train/test example? These examples will
	have no effect on another example for some methods.

Optimal Counterfactual Explanations in Tree Ensembles
- Finds most similar example to test example that can flip or change its
	predicted label.

- Create script to plot leaf weight vs boost no. (binary, regression) for a single
	test example, or avg. for multiple test examples.

- Analyzing the counterfactual influence set only makes sense for these explainers
	when the prediction is CORRECT; if it is incorrect, these explainers
	will increase log loss by removing examples from the non-predicted class.

	* There is NO FLIPPING of the label when the prediction is incorrect!