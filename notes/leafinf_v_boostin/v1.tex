\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}


\begin{document}

\subsection*{LeafInfluence vs. BoostIn}

- Consider a two-tree ensemble. \\
- Assume fixed structure. \\
- Analysis for a single train example $i$.

\subsubsection*{Tree 1}

\emph{LeafInfluence} (Single Point):

\begin{align}
\frac{\partial f_{H;l}^t}{\partial w_i} &= \frac{I_l^t(i) (h_i^t f_{H;l}^t + g_i^t) + \sum_{j \in I_l^t} w_j (k_j^t f_{H;l}^t + h_j^t) J(A^{t-1})_{ij}}{H_{H;l}^t}
\end{align}

\begin{align*}
f_{H;l}^t &= \alpha_l^t = \text{leaf value for leaf $l$ in tree $t$.} \\
w_j &= 1, \forall j. \\
J(A^{t-1})_{ij} &= 0 \text{ $\forall$ i, j; initialization.} \\
\text{Also, } J(A^{t-1})_{ij} &= 0 \text{ when } i \neq j, \forall t. \\
\end{align*}

Thus, eq. (1) becomes:
\begin{align}
\frac{\partial \alpha_l^t}{\partial w_i} &= \frac{h_i^t \alpha_l^t + g_i^t}{H_{H;l}^t} \\
\frac{\partial \alpha_l^t}{\partial w_i} &= 0, \forall l \text{ in which } I_l^t(i) = 0. \nonumber
\end{align}

Update prediction derivative for train example $i$:
\begin{align*}
J(A^t)_{ii} = J(A^{t-1})_{ii} + eq. (2)
\end{align*}

\emph{Note}: In the code, eq. (2) is:
\begin{align}
\frac{\partial \alpha_l^t}{\partial w_i} &= \frac{h_i^t \alpha_l^t / \eta + g_i^t}{H_{H;l}^t / \eta} * \eta, \text{ $\eta$ = learning rate.} \nonumber
\end{align}
\\
\emph{BoostIn}:
\begin{align}
\frac{g_i^t}{n_l^t} * \eta, \text{ $n_l^t$ = no. examples at leaf $l$ for tree $t$.} \nonumber
\end{align}

\subsubsection*{Tree 2}

\emph{LeafInfluence} (Single Point):

\begin{align}
\frac{\partial \alpha_l^t}{\partial w_i} &= \frac{(h_i^t \alpha_l^t + g_i^t) + (k_j^t \alpha_l^t + h_j^t) J(A^{t-1})_{ii}}{H_{H;l}^t}
\end{align}

Update prediction derivative for train example $i$:
\begin{align*}
J(A^t)_{ii} = J(A^{t-1})_{ii} + eq. (3)
\end{align*}

\emph{Note}: In the code, eq. (3) is:
\begin{align}
\frac{\partial \alpha_l^t}{\partial w_i} &= \frac{(h_i^t \alpha_l^t / \eta + g_i^t) + (k_j^t \alpha_l^t / \eta + h_j^t) J(A^{t-1})_{ii}}{H_{H;l}^t / \eta} * \eta \nonumber
\end{align}
\\
\emph{BoostIn}:
\begin{align}
\frac{g_i^t}{n_l^t} * \eta \nonumber
\end{align}

\subsubsection*{Approximating Test Loss}

\emph{LeafInfluence} (Single Point):
\begin{align*}
Inf(x_i, x_\tau) &= -\nabla \left(\sum_t \alpha_{P(x_\tau)_t}^t\right) \left(\sum_t \frac{\partial \alpha_{P(x_\tau)_t}^t}{\partial w_i}\right)
\\
x_\tau &= \text{ test example.}
\\
P(x_\tau)_t &= \text{ leaf that $x_\tau$ ends at for tree $t$.}
\end{align*}
\\
\emph{BoostIn}:
\begin{align*}
Inf(x_i, x_\tau) = \sum_t I[P(x_i)_t = P(x_\tau)_t] \frac{g_i^t g_\tau^t}{n_l^t} * \eta
\end{align*}

\subsubsection*{Complexity Analysis}

\emph{LeafInfluence} (Single Point):
\\
- Precompute 1st, 2nd, and 3rd derivatives.
\\
- Needs to keep track of prediction derivatives, one for each train example.
\\
- Can compute leaf derivatives for all train examples in one pass.
\\
- Complexity: O(Tn).
\\
\\
\emph{BoostIn}:
\\
- Precompute 1st derivatives.
\\
- Complexity: O(Tn).

\subsubsection*{Additional Notation}
\begin{align*}
g_i^t &= \text{ 1st derivative for example $i$ in tree $t$.} \\
h_i^t &= \text{ 2nd derivative for example $i$ in tree $t$.} \\
k_i^t &= \text{ 3rd derivative for example $i$ in tree $t$.} \\
H_{H;l}^t &= \text{ Sum of 2nd derivatives for leaf $l$ in tree $t$.} \\
J(A^t) &= \text{ Jacobian prediction interaction matrix for tree $t$.} \\
I_l^t &= \text{ Set of train examples at leaf $l$ for tree $t$.} \\
I_l^t(i) &= \text{ 1 if example $i$ is at leaf $l$ for tree $t$, 0 otherwise.}
\end{align*}

\end{document}
