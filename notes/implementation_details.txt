Primers on gradient boosting and gradient boosted decision trees:
https://xgboost.readthedocs.io/en/latest/tutorials/model.html
https://everdark.github.io/k9/notebooks/ml/gradient_boosting/gbt.nb.html
https://explained.ai/gradient-boosting/

Catboost

Default leaf_estimation_method: Newton
Default leaf reg. (l2_leaf_reg): 3
Default learning rate: 0.3

Leaf estimate computation (for Newton): sum(gradients) / (sum(hessian) + leaf. reg.) * learning rate
	- For multi class, sum gradients and hessians along axis=0.
Need to set leaf_estimation_iterations=1 to be consistent with other implementations
Need to make sure leaf_estimation_method='Newton'

Binary Classifier
	Default initial prediction: 0.0 (log odds in log space; 0.5 y_hat)
 	Default loss function: log loss (logistic loss)
	Gradient_i: y_hat[i] - y[i]
	Hessian_i: y_hat[i] * (1 - y_hat[i])

Multiclass Classifier  (NOTE: leaf estimates are not super precise)
	Default initial prediction: 0.0 for each class (uniform log probs.; 1/n_class y_hat)
	Default loss function: MultiClass
	Gradient_i for tree of class k: (y_hat[i] - 1) if y[i] == k, otherwise yhat[i]
	Hessian_i: factor_ * k_prob. * (1 - k_prob.)
		factor_ = (no. classes) / (no. classes - 1)
	Note: Multiclass predictions and leaf estimations are pretty imprecise,
		typically precise to about 0.01 tolerance.

Regression
	Default initial prediction: mean
	Default loss function: RMSE (squared error)
	Gradient_i: y_hat[i] - y[i]
	Hessian_i: 1


XGBoost

Leaf weight is the hessian sum.
max_delta_step is a hard limit on the leaf update amount; learning rate still applies to this update amount.
	Not usually needed unless classes are highly imbalanced.

Default leaf estimation method: Newton
Default l1 reg. (reg_alpha): 0.0
Default l2 reg. (leaf reg.) (reg_lambda): 1.0
Default learning rate: 0.3

Leaf estimate computation (for Newton): sum(gradients) / (sum(hessian) + leaf. reg.) * learning rate

Binary Classifier
	Default initial prediction: 0.5 (0.0 log odds in log space; 0.5 y_hat, could be extracted from 'base_score' parameter)
 	Default loss function: logistic loss
	Gradient_i: y_hat[i] - y[i]
	Hessian_i: y_hat[i] * (1 - y_hat[i])

Multiclass Classifier TODO: can't replicate leaf estimates...
	Default initial prediction: 0.5 (0.0 uniform log probs.; 1/n_class y_hat, CANNOT be extracted from 'base_score' parameter)
	Default loss function: multi:softprob
	Gradient_i for tree of class k: (y_hat[i] - 1) if y[i] == k, otherwise yhat[i]
	Hessian_i: factor_ * k_prob. * (1 - k_prob.)
		factor_ = 2

Regression
	Default initial prediction: 0.5 (I don't know why, seems arbitrary, extracted from 'base_score' parameter)
	Default loss function: reg:squarederror (squared error)
	Gradient_i: y_hat[i] - y[i]
	Hessian_i: 1


LightGBM

Gradient computations are defined in these loss functions:
https://github.com/microsoft/LightGBM/tree/master/src/objective

Leaf weight is the hessian sum.
max_delta_step is a hard limit on the leaf update amount; learning rate still applies to this update amount.
	Not usually needed unless classes are highly imbalanced.

Default leaf estimation method: Newton
Default l1 reg. or gradient sum reg. (reg_alpha): 0.0
Default l2 reg. or hessian sum reg. (reg_lambda): 0.0
Default max_leaf_output (max_delta_step, max_tree_output): 0.0  (make sure this is zero for the unified implementation)
Default learning rate: 0.1
Default sigmoid parameter: 1.0 (make sure this is 1 in the unified implementation)
Default class weights: None (make sure this is None)
Default instance weights: None (make sure this is None)

Binary Classifier
	Default initial prediction: class prior (logit(pos. class prob.) baked into the leaf values of the first tree)
 	Default loss function: binary sigmoid:1 (logistic loss)
	First Tree: Leaf estimate computation (for Newton): -(max(0, sum(gradients) - l1) / (sum(hessian) + l2) * learning rate) + logit(pos. class prob.)
	Subsequent Trees: Leaf estimate computation (for Newton): -max(0, sum(gradients) - l1) / (sum(hessian) + l2) * learning rate
	Gradient_i: y_hat[i] - y[i]
	Hessian_i: y_hat[i] * (1 - y_hat[i])

Multiclass Classifier
	Default initial prediction: (log(k class prob.) baked into each respective class tree.
	Default loss function: multiclass (softmax + CE)
	First Tree: Leaf estimate computation (for Newton): -(max(0, sum(gradients) - l1) / (sum(hessian) + l2) * learning rate) + log(k class prob.)
	Subsequent Trees: Leaf estimate computation (for Newton): -max(0, sum(gradients) - l1) / (sum(hessian) + l2) * learning rate)
	Gradient_i for tree of class k: (y_hat[i] - 1) if y[i] == k, otherwise yhat[i]
	Hessian_i: factor_ * k_prob. * (1 - k_prob.)
	factor_ = no. class / (no. class - 1); rescales the redundant form of K-classification to the non-redundant
		form. In the redundant form, there is a redundant class (class 0 in binary classification). This is from the Friedman GBDT paper.

Regression
	Default initial prediction: (mean(y) baked into the leaf values of the first tree)
	Default loss function: regression (squared error)
	First Tree: Leaf estimate computation (for Newton): -max(0, sum(gradients) - l1) / (sum(hessian) + l2) * learning rate) + mean(y)
	Subsequent Trees: Leaf estimate computation (for Newton): -max(0, sum(gradients) - l1) / (sum(hessian) + l2) * learning rate)
	Gradient_i: y_hat[i] - y[i]
	Hessian_i: 1

To make lightgbm consistent with the other implementations, subtract initial guess from first tree leaf values, and use the
initial guess as the bias term in the unified representation across all implementations.


SKGBM

Leaf values are updated using these loss classes:
https://github.com/scikit-learn/scikit-learn/blob/2beed55847ee70d363bdbfe14ee4401438fba057/sklearn/ensemble/_gb_losses.py

Default leaf estimation method: Newton
Default leaf reg. (no name?): 0.0
Default learning rate: 0.1

Learning rate is not multiplied into leaf values, it's multiplied in during prediction.

Binary Classifier
	Default initial prediction: log-odds ratio (class priors in log space; class prior prob. y_hat)
 	Default loss function: binomial deviance (logistic loss = sigmoid + binary CE)
	Leaf estimate computation (for Newton): -(sum(gradients) / sum(hessian))
	Gradient_i: y_hat[i] - y[i]
	Hessian_i: y_hat[i] * (1 - y_hat[i])

Multiclass Classifier
	Default initial prediction: class priors (class priors in prob. space; class prior prob. y_hat)
	Default loss function: multinomial deviance (softmax + CE)
	Leaf estimate computation (for Newton): -(sum(gradients) / sum(hessian))
	Gradient_i for tree of class k: (y_hat[i] - 1) if y[i] == k, otherwise yhat[i]
	Hessian_i: factor_ * k_prob. * (1 - k_prob.)
		factor_ = (no. classes) / (no. classes - 1)

Regression
	Default initial prediction: mean
	Default loss function: squared error
	Leaf estimate computation (for Newton): -(sum(gradients) / sum(hessian))
	Gradient_i: y_hat[i] - y[i]
	Hessian_i: 1


NOTE:
	- Newton-Raphson step for least squares loss is equivalent to taking the mean
	of the target values for the examples that end at that leaf since the
	hessian of least squares is just 1.


SKRF

When boostrap=True, then it appears there is no way to tell which examples (and more importantly)
	and how many of each example are used to estimate each leaf value.

Thus, for this work, we need bootstrap=False.

Binary Classifier
	Default initial prediction: None (0)
	Leaf estimate computation: Originally counts for each class (neg. and pos.).
		Updated leaf estimate: We normalize the counts to get probabilities for the pos. class.

Multiclass Classifier
	Default initial prediction: None (0)
	Leaf estimate computation: Originally counts for each class (k classes).
		Updated leaf estimates: We normalize the counts to get prob. for each class.

Regression
	Default initial prediction: None (0)
	Leaf estimate computation: Mean of the targets for the examples at that leaf.
